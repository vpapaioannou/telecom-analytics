# Distribution Computation

This is the second step of the [Peak Detection](../../../../docs/PeakDetection.md) calculation
and it requires input generated by the [Data Filter] (../dataFilter/README.md) step.

Based on the densities obtained for each region and each time slot over the training dataset, an expected density value is computed for each region, by averaging the densities measured at the same time slot of all the periods in the time window covered by the dataset.
For instance, we might obtain an expected density for each pair (region, hour of the day), i.e., 24 values for each region, assuming 24 one-hour time-slots.
The result represents the standard behavior and it is saved in a new dataset named cpBase.

## Implementation Details

As an spark application, this operator can be executed by being submitted in an running spark installation.
For simplifying the execution [submit.sh] (../../../../submit.sh) can be used.

**Usage**: ./submit.sh ta.DistributionComputation \<master\> \<trainingDataFile\> \<output\>


**Input parameters**:
- the spark master URI
- the training dataset URI (HDFS or local) (created during the data filtering step)
- the ouput path (an non existing HDFS or local directory)

**Output**:
Upon successful execution the \<output\>/cpBase dataset will be created.

e.g.: ./submit.sh ta.DistributionComputation spark://localhost:7077 /output/trainingData /output

## SQL formalization:

    //The table representing the standard behavior, in this case it is computed on the first month of the year. The aggregation reduce everything in a single week (the day of the weeks are not mixed)
    create table cp_base as
    Select rid,hour,dow, avg(num) as num from training_data
    group by rid,hour,dow;
